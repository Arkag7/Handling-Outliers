{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad628b6",
   "metadata": {},
   "source": [
    "#### There are three different types of missing data\n",
    "\n",
    "1) Missing completely at random (MCAR)\n",
    "2) Missing at random (MAR)\n",
    "3) Not missing at random (NMAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9653a1",
   "metadata": {},
   "source": [
    "#### popular ways for data imputation for cross-sectional datasets\n",
    "Source:\n",
    "    \n",
    "https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779\n",
    "    \n",
    "    \n",
    "###  1. Do Nothing:\n",
    "\n",
    "We can just let the algorithm handle the missing data. Some algorithms can factor in the missing values and learn the \n",
    "best imputation values for the missing data based on the training loss reduction (ie. XGBoost). \n",
    "Some others have the option to just ignore them (ie. LightGBM — use_missing=false). However, other algorithms will panic and \n",
    "throw an error complaining about the missing values (ie. Scikit learn — LinearRegression). \n",
    "In that case, you will need to handle the missing data and clean it before feeding it to the algorithm.\n",
    "\n",
    "### 2. Imputation Using (Mean/Median) Values:\n",
    "\n",
    "This works by calculating the mean/median of the non-missing values in a column and then replacing the missing values within \n",
    "each column separately and independently from the others. It can only be used with numeric data.\n",
    "\n",
    "#### Pros:\n",
    "Easy and fast.\n",
    "Works well with small numerical datasets.\n",
    "\n",
    "#### Cons:\n",
    "Doesn’t factor the correlations between features. It only works on the column level.\n",
    "Will give poor results on encoded categorical features (do NOT use it on categorical features).\n",
    "Not very accurate.\n",
    "Doesn’t account for the uncertainty in the imputations.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5527cd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20640\n",
      "20640\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "import random\n",
    "import numpy as np\n",
    "random.seed(0)\n",
    "\n",
    "#Fetching the dataset\n",
    "import pandas as pd\n",
    "dataset = fetch_california_housing()\n",
    "train, target = pd.DataFrame(dataset.data), pd.DataFrame(dataset.target)\n",
    "train.columns = ['0','1','2','3','4','5','6','7']\n",
    "train.insert(loc=len(train.columns), column='target', value=target)\n",
    "\n",
    "#Randomly replace 40% of the first column with NaN values\n",
    "column = train['0']\n",
    "print(column.size)\n",
    "missing_pct = int(column.size * 0.4)\n",
    "i = [random.choice(range(column.shape[0])) for _ in range(missing_pct)]\n",
    "column[i] = np.NaN\n",
    "print(column.shape[0])\n",
    "\n",
    "#Impute the values using scikit-learn SimpleImpute Class\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp_mean = SimpleImputer( strategy='mean') #for median imputation replace 'mean' with 'median'\n",
    "imp_mean.fit(train)\n",
    "imputed_train_df = imp_mean.transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1df38a9",
   "metadata": {},
   "source": [
    "### 3. Imputation Using (Most Frequent) or (Zero/Constant) Values:\n",
    "Most Frequent is another statistical strategy to impute missing values and YES!! It works with categorical features \n",
    "(strings or numerical representations) by replacing missing data with the most frequent values within each column.\n",
    "    \n",
    "#### Pros:\n",
    "Works well with categorical features.\n",
    "\n",
    "#### Cons:\n",
    "It also doesn’t factor the correlations between features.\n",
    "It can introduce bias in the data.\n",
    "\n",
    "Zero or Constant imputation — as the name suggests — it replaces the missing values with either zero or any constant \n",
    "value you specify\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a09b992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute the values using scikit-learn SimpleImpute Class\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp_mean = SimpleImputer( strategy='most_frequent')\n",
    "imp_mean.fit(train)\n",
    "imputed_train_df = imp_mean.transform(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f710023",
   "metadata": {},
   "source": [
    "#### 4. Imputation Using k-NN:\n",
    "    \n",
    "The algorithm uses ‘feature similarity’ to predict the values of any new data points. This means that the new point is assigned \n",
    "a value based on how closely it resembles the points in the training set. This can be very useful in making predictions about \n",
    "the missing values by finding the k’s closest neighbours to the observation with missing data and then imputing them based \n",
    "on the non-missing values in the neighbourhood. \n",
    "\n",
    "It creates a basic mean impute then uses the resulting complete list to construct a KDTree. Then, it uses the resulting KDTree \n",
    "to compute nearest neighbours (NN). After it finds the k-NNs, it takes the weighted average of them\n",
    "\n",
    "#### Pros:\n",
    "Can be much more accurate than the mean, median or most frequent imputation methods (It depends on the dataset).\n",
    "\n",
    "#### Cons:\n",
    "Computationally expensive. KNN works by storing the whole training dataset in memory.\n",
    "K-NN is quite sensitive to outliers in the data (unlike SVM)\n",
    "\n",
    "Let's discuss KNN Imputer with an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7664af9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\10635638\\anaconda3\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\10635638\\anaconda3\\lib\\site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\10635638\\anaconda3\\lib\\site-packages (from scikit-learn) (1.6.2)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\10635638\\anaconda3\\lib\\site-packages (from scikit-learn) (1.22.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\10635638\\anaconda3\\lib\\site-packages (from scikit-learn) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8af2585",
   "metadata": {},
   "source": [
    "#### How does KNN Imputer work?\n",
    "According scikit-learn docs: Each sample’s missing values are imputed using the mean value from n_neighbors nearest neighbors found in the training set. Two samples are close if the features that neither is missing are close. By default, a euclidean distance metric that supports missing values, nan_euclidean_distances, is used to find the nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7eb4fc5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first</th>\n",
       "      <th>second</th>\n",
       "      <th>Third</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>112.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>56.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>89.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   first  second  Third\n",
       "0  112.0    30.0    NaN\n",
       "1   90.0    45.0   40.0\n",
       "2    NaN    56.0   80.0\n",
       "3   89.0     NaN   98.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Dataframe with Missing Values\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df= {'first': [112, 90, np.nan, 89],\n",
    "    'second': [30,45,56, np.nan],\n",
    "    'Third':[np.nan, 40, 80, 98]}\n",
    "\n",
    "df= pd.DataFrame(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86ba6772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[112. ,  30. ,  69. ],\n",
       "       [ 90. ,  45. ,  40. ],\n",
       "       [100.5,  56. ,  80. ],\n",
       "       [ 89. ,  43. ,  98. ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 Initialize KNNImputer\n",
    "# You can define your own n_neighbors value (as its typical of KNN algorithm)\n",
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "\n",
    "df_filled = imputer.fit_transform(df)\n",
    "df_filled"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
